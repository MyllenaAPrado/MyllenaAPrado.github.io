---
---


@INPROCEEDINGS{11084318,
  author={Prado, Myllena A. and Farias, Mylène C. Q.},
  booktitle={2025 IEEE International Conference on Image Processing (ICIP)}, 
  title={NASSBLiF: No-Reference Light Field Image Quality Assessment Via Neighborhood Attention and Scale Swin}, 
  year={2025},
  volume={},
  number={},
  code={https://github.com/MyllenaAPrado/NASSBLIF},
  pages={659-664},
  selected={true},
  keywords={Image quality;Measurement;Attention mechanisms;Computational modeling;Virtual environments;Feature extraction;Transformers;Light fields;User experience;Computational efficiency;Light Field;Image Quality Assessment;Swin Transformer;Neighborhood attention},
  doi={10.1109/ICIP55913.2025.11084318}
  }


@inproceedings{10.1145/3539637.3557930,
author = {Prado, Myllena and Althoff, Lucas and Alamgeer, Sana and Silva, Alessandro Rodrigues e and Prakash, Ravi and Carvalho, Marcelo M. and Farias, Myl\`{e}ne C. Q.},
title = {360RAT: A Tool for Annotating Regions of Interest in 360-degree Videos},
year = {2022},
isbn = {9781450394093},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539637.3557930},
doi = {10.1145/3539637.3557930},
abstract = {This paper introduces the software 360RAT as a tool for annotating regions of interest (RoIs) in 360-degree videos. These regions represent the portions of the video content that are important for telling a story throughout the video. We believe that this software is an invaluable tool for studying different aspects of 360-degree videos, including what viewers consider relevant and interesting to the experience. As part of this work, we conducted a subjective experiment in which 9 human observers used the proposed software to annotate 11 360-degree videos. As a result, we created a dataset containing a set of annotated 360-degree videos, i.e., videos with marked RoIs and their semantic classification. We present a simple analysis of the annotations gathered with the experiment for a subset of the videos. We noticed that there is a higher agreement of annotations among participants for videos with fewer objects. We also compared the RoI maps with saliency maps computed with the Cube Padding saliency model. We found a strong correlation between RoI maps and computed saliency models, indicating a link between the annotated RoI and the saliency properties of the content.},
booktitle = {Proceedings of the Brazilian Symposium on Multimedia and the Web},
pages = {272–280},
numpages = {9},
selected={true},
code={https://github.com/MyllenaAPrado/360RAT},
keywords = {360-degree video, attention guiding, cinematic VR, user experience},
location = {Curitiba, Brazil},
series = {WebMedia '22}
}


@article{doi:10.2352/EI.2022.34.9.IQSP-394,
author = {Lucas dos Santos Althoff and Henrique D. Garcia and Dario D. R. Morais and Sana Alamgeer and Myllena A. Prado and Gabriel C. Araujo and Ravi Prakash and Marcelo M. Carvalho and Mylène C. Q. Farias},
title = {Designing an user-centric framework for perceptually-efficient streaming of 360&#xB0; edited videos},
journal = {Electronic Imaging},
volume = {34},
number = {9},
pages = {394-1--394-1},
keywords = {360-degree videos, visual attention, 360-degree video streaming, video edition},
doi = {10.2352/EI.2022.34.9.IQSP-394},
url = {https://library.imaging.org/ei/articles/34/9/IQSP-394},
year = {2022},
}


